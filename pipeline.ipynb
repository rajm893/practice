{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc23e25-2895-442e-950e-ba5e70021586",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SKY Assignment\n",
    "\n",
    "All the below steps are peformed in the notebook:\n",
    "1. Update the model specification to include an additional convolutional layer containing 16 filters with a kernel size of 3.\n",
    "2. Add a max pooling layer after each of the convolutional layers to reduce the dimensionality of the outputs passed to the next layer.\n",
    "3. Add a callback to save a model checkpoint after every training epoch.\n",
    "4. Define a simple Vertex AI pipeline that:\n",
    "    1. Trains the model as specified in part (3).\n",
    "    2. Uploads the trained model to Vertex AI as a model resource.\n",
    "    3. Deploys the model resource to the an endpoint resource.\n",
    "5. Provide code to compile and run the pipeline.\n",
    "\n",
    "## Introduction\n",
    "This notebook outlines the process to train a deep learning model on the CIFAR-10 dataset using TensorFlow. The model will then be deployed to Vertex AI.\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "- Ensure you have access to a Google Cloud project with Vertex AI, Artifact Registry and Google Cloud Storage API enabled. \n",
    "\n",
    "## Setup\n",
    "- First, we import necessary libraries and set up environment variables. Google Cloud credentials should be correctly configured for this notebook to interact with Google Cloud resources.\n",
    "- To begin with, I have created a service account in IAM service account with owner role and downloaded the json key credentials file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9026b85e-7d0a-4e0e-adf5-fb7c29ad372e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import (component, \n",
    "                     Metrics,\n",
    "                     OutputPath,\n",
    "                     Output)\n",
    "import google.cloud.aiplatform as aiplatform\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"myproject-220424-39ec352fe30c.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e7b7fc-b6c8-42e1-9276-11ac7fb63799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud-ai-platform-973f4c90-d1ae-41e0-908a-311b0ed228b9\n",
      "sky-assign-bucket-final-23042024\n",
      "sky-project-bucket-22042024\n",
      "sky_assign_bucket_23042024\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "# List buckets\n",
    "for bucket in storage_client.list_buckets():\n",
    "    print(bucket.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe2d42-9843-499c-a4b3-21d6be82779c",
   "metadata": {},
   "source": [
    "## Define Parameters\n",
    "- Here we define parameters like project name, bucket name, REGION, PIPELINE_ROOT and BASE_IMAGE. These should be updated according to your Google Cloud setup.\n",
    "- I have built and pushed the BASE_IMAGE image to Google Artifact Registry. Please find the code for build and push the image in \"docker_build.sh\" and \"docker_push_gcr.sh\" file. Also find the requirements.txt file which includes the dependencies. \n",
    "  - Create a repository in Google artifact registry for storing the docker images.\n",
    "  - Make sure edit the parameters such as PROJECT_ID, REGION, REPOSITORY, IMAGE and IMAGE_TAG.\n",
    "  - Run \"docker_build.sh\" and then Run \"docker_push_gcr.sh\"\n",
    "- Once Docker image is pushed we can define the base image path (mentioned in docker_push_gcr.sh) below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c49391a-e337-4d04-922f-566506c18a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"sky-assign-bucket-final-23042024\"\n",
    "PROJECT_NAME = \"myproject-220424\"\n",
    "REGION = \"europe-west2\"\n",
    "BASE_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_NAME}/sky-assign/training:latest\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c0ee3-d61b-4bd4-99a1-86383437641d",
   "metadata": {},
   "source": [
    "## Define Training Component\n",
    "- This component is responsible for training the model using TensorFlow.\n",
    "- Create a folder named 'checkpoints' in the same bucket.\n",
    "- It involves:\n",
    "    - Data loading\n",
    "    - Preprocessing\n",
    "    - Model Training\n",
    "    - Checkpoint Callback to save a model checkpoint after every training epoch\n",
    "    - Model Saving\n",
    "    - returns the model path that further used in deploy component \n",
    "\n",
    "- This training script not only trains the model but also logs training and validation metrics, evaluates the model on a test dataset, and saves the model to Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e20671-d476-4e19-ab9d-bc2e06d91f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def train_model(project: str, bucket_name: str, model_path: OutputPath(str), metrics: Output[Metrics]) -> str:\n",
    "    import tensorflow_datasets as tfds\n",
    "    import tensorflow as tf\n",
    "    import os\n",
    "    \n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255.0\n",
    "        return image, label\n",
    "\n",
    "    BUFFER_SIZE = 10000\n",
    "    BATCH_SIZE = 64\n",
    "    VAL_SPLIT = 0.2 \n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "        metrics=['accuracy'])    \n",
    "    \n",
    "    # Model checkoint \n",
    "    checkpoint_path = f'gs://{bucket_name}/checkpoints'\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath= os.path.join(checkpoint_path,'epoch_{epoch}'),\n",
    "    save_best_only=False)\n",
    "    \n",
    "    # Load the dataset \n",
    "    datasets, info = tfds.load(name='cifar10', with_info=True, as_supervised=True)\n",
    "    \n",
    "    # Split the dataset to train, validation and test set\n",
    "    train_data = datasets['train'].map(scale).shuffle(BUFFER_SIZE)\n",
    "    val_size = int(VAL_SPLIT * info.splits['train'].num_examples)\n",
    "    train_size = info.splits['train'].num_examples - val_size\n",
    "    train_dataset = train_data.take(train_size).repeat().batch(BATCH_SIZE)\n",
    "    val_dataset = train_data.skip(train_size).batch(BATCH_SIZE)\n",
    "    test_dataset = datasets['test'].map(scale).batch(BATCH_SIZE)\n",
    "    \n",
    "    # Model training\n",
    "    history = model.fit(\n",
    "        x=train_dataset,\n",
    "        epochs=10,\n",
    "        steps_per_epoch=train_size // BATCH_SIZE,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[checkpoint_callback])\n",
    "    \n",
    "    # Logging train metrics \n",
    "    for metric, values in history.history.items():\n",
    "        metrics.log_metric('train_'+metric, float(values[-1]))\n",
    "    \n",
    "    # Evaluate the model on the test set and logging test metrics\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "    metrics.log_metric(\"test_loss\", float(test_loss))\n",
    "    metrics.log_metric(\"test_accuracy\", float(test_accuracy))\n",
    "    \n",
    "    # Save the model in the GCP storage bucket\n",
    "    model_dir = f\"gs://{bucket_name}/models/\"\n",
    "    model_path_str = os.path.join(model_dir, \"my_model\")\n",
    "    model.save(model_path_str)\n",
    "\n",
    "    # Write the model path to the output path\n",
    "    with open(model_path, 'w') as f:\n",
    "        f.write(model_path_str)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216fa41-1c2f-4da3-9ae0-9e67aca19ed4",
   "metadata": {},
   "source": [
    "## Define Deployment Component\n",
    "- After training, this component is responsible for deploying the trained model to Vertex AI for serving predictions. \n",
    "    - The model is uploaded as a Vertex AI Model resource.\n",
    "    - Then deployed to an endpoint.\n",
    "- This component uploads the trained model to Vertex AI and creates an endpoint for it. The model is then deployed to this endpoint, making it ready for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a52ee5aa-10c0-4f59-9ff5-9a090eed152a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(base_image=BASE_IMAGE)\n",
    "def deploy_model(project: str, location: str, model_path: str):\n",
    "    from google.cloud.aiplatform import Model\n",
    "    from google.cloud.aiplatform import Endpoint\n",
    "    from datetime import datetime\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    model = Model.upload(\n",
    "        display_name=\"cifar10_model\",\n",
    "        artifact_uri=model_path,\n",
    "        serving_container_image_uri=\"europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-10:latest\"\n",
    "    )\n",
    "    endpoint = Endpoint.create(display_name=\"cifar10_endpoint\")\n",
    "    # Upload model to vertex ai model resourse\n",
    "    endpoint.deploy(model=model, deployed_model_display_name='\"deployed_cifar10_model_{}\"'.format(TIMESTAMP), machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3ed0e-4af0-47df-a7d2-723d5574879c",
   "metadata": {},
   "source": [
    "## Define and Compile the Pipeline\n",
    "- This pipeline orchestrates the training and deployment process. It defines the workflow where the model is first trained and then deployed.\n",
    "- This code snippet defines a Kubeflow Pipeline that first trains a CIFAR-10 model and then deploys it. The compiled pipeline can be submitted to Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ba4589-0c4a-447f-8b59-b665a39e9e1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "@dsl.pipeline(name=\"cifar10-training-deployment\",\n",
    "              pipeline_root = PIPELINE_ROOT)\n",
    "\n",
    "def pipeline(project: str, bucket_name: str):\n",
    "    train_op = train_model(project=PROJECT_NAME, bucket_name=BUCKET_NAME)\n",
    "    deploy_model(project=PROJECT_NAME, location=\"eu-west2\", model_path=train_op.outputs['model_path'])\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"cifar10_pipeline.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ce6d6-cd47-49b5-8d2f-6f673784f8d5",
   "metadata": {},
   "source": [
    "## Execute the Pipeline\n",
    "- Finally, execute the compiled pipeline on Vertex AI. This step requires specifying the project, region, and other necessary parameters.\n",
    "- This command initiates the pipeline run on Vertex AI, where it executes according to the defined tasks—training the model and then deploying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a355817-120a-44d2-992b-317d3a68c1e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/europe-west2/pipelines/runs/cifar10-training-deployment-20240424120326?project=1009435750450\n",
      "PipelineJob projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/1009435750450/locations/europe-west2/pipelineJobs/cifar10-training-deployment-20240424120326\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform import PipelineJob\n",
    "\n",
    "# Parameters\n",
    "project_id = PROJECT_NAME\n",
    "bucket_name = BUCKET_NAME\n",
    "region = REGION\n",
    "\n",
    "pipeline_job = PipelineJob(\n",
    "    display_name=\"cifar10_pipeline\",\n",
    "    template_path=\"cifar10_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'project': project_id,\n",
    "        'bucket_name': bucket_name\n",
    "    },\n",
    "    location=region,\n",
    ")\n",
    "\n",
    "pipeline_job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e64c52d-810c-4b63-85c9-7bb2634beb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.10 (Local)",
   "language": "python",
   "name": "tf2-2-10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
